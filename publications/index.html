<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Michael Lopez-Brau</title> <meta name="author" content="Michael Lopez-Brau"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lopez-brau.github.io/publications/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Michael </span>Lopez-Brau</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/cairn.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="cairn.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lopezbrau2023social" class="col-sm-8"> <div class="title">The Social Representation of the Physical World</div> <div class="author"> <em>Michael Lopez-Brau</em> </div> <div class="periodical"> <em>Yale University</em>, 2023 </div> <div class="periodical"> PhD dissertation </div> <div class="links"> <a href="https://lopez-brau.github.io/assets/pdf/dissertation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/predicted_agent_behavior.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="predicted_agent_behavior.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lopezbrau2023people" class="col-sm-8"> <div class="title">People can use the placement of objects to infer communicative goals</div> <div class="author"> <em>Michael Lopez-Brau</em> and <a href="https://psychology.yale.edu/people/julian-jara-ettinger" rel="external nofollow noopener" target="_blank">Julian Jara-Ettinger</a> </div> <div class="periodical"> <em>Cognition</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://lopez-brau.github.io/assets/pdf/Lopez-Brau%20&amp;%20Jara-Ettinger%20(2023).pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/lopez-brau/physical_pragmatics" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.1016/j.cognition.2023.105524"></span> <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.1016/j.cognition.2023.105524" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Beyond words and gestures, people have a remarkable capacity to communicate indirectly through everyday objects: A hat on a chair can mean it is occupied, rope hanging across an entrance can mean we should not cross, and objects placed in a closed box can imply they are not ours to take. How do people generate and interpret the communicative meaning of objects? We hypothesized that this capacity is supported by social goal inference, where observers recover what social goal explains an object being placed in a particular location. To test this idea, we study a category of common ad-hoc communicative objects where a small cost is used to signal avoidance. Using computational modeling, we first show that goal inference from indirect physical evidence can give rise to the ability to use object placement to communicate. We then show that people from the U.S. and the Tsimane’—a farming-foraging group native to the Bolivian Amazon—can infer the communicative meaning of object placement in the absence of a pre-existing convention, and that people’s inferences are quantitatively predicted by our model. Finally, we show evidence that people can store and retrieve this meaning for use in subsequent encounters, revealing a potential mechanism for how ad-hoc communicative objects become quickly conventionalized. Our model helps shed light on how humans use their ability to interpret other people’s behavior to embed social meaning into the physical world.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lopezbrau2023people</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{People can use the placement of objects to infer communicative goals}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lopez-Brau, Michael and Jara-Ettinger, Julian}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Cognition}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{239}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{105524}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.cognition.2023.105524}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/path_reconstructions.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="path_reconstructions.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lopezbrau2022social" class="col-sm-8"> <div class="title">Social inferences from physical evidence via Bayesian event reconstruction</div> <div class="author"> <em>Michael Lopez-Brau</em>, Joseph Kwon, and <a href="https://psychology.yale.edu/people/julian-jara-ettinger" rel="external nofollow noopener" target="_blank">Julian Jara-Ettinger</a> </div> <div class="periodical"> <em>Journal of Experimental Psychology: General</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://lopez-brau.github.io/assets/pdf/Lopez-Brau%20et%20al.%20(2022).pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/lopez-brau/ImageInference" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://psycnet.apa.org/doi/10.1037/xge0001182"></span> <span class="__dimensions_badge_embed__" data-doi="https://psycnet.apa.org/doi/10.1037/xge0001182" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Humans can make remarkable social inferences by watching each other’s behavior. In many cases, however, people can also make social inferences about agents whose behavior they cannot see, based only on the physical evidence left behind. We hypothesized that this capacity is supported by a form of mental event reconstruction. Under this account, observers derive social inferences by reconstructing the agent’s behavior, based on the physical evidence that revealed their presence. We present a computational model of this idea, embedded in a Bayesian framework for action understanding, and show that its predictions match human inferences with high quantitative accuracy. Specifically, Experiment 1 shows that people can infer where an agent came from and which goal they pursued in a room, all from a small pile of cookie crumbs. Experiment 2 shows that people can explicitly reconstruct the actions that the agent took, and these reconstructed trajectories can predict the entry point and goal inferences from Experiment 1. Finally, Experiment 3 shows that people can also infer whether one or two agents were in a room based on the position of two piles of cookie crumbs. Our results shed light on how people extract social information from the physical world.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lopezbrau2022social</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Social inferences from physical evidence via Bayesian event reconstruction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lopez-Brau, Michael and Kwon, Joseph and Jara-Ettinger, Julian}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Experimental Psychology: General}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{151}</span><span class="p">,</span>
  <span class="na">issue</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2029--2042}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{American Psychological Association}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://psycnet.apa.org/doi/10.1037/xge0001182}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/block_towers.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="block_towers.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lopezbrau2021attentional" class="col-sm-8"> <div class="title">Attentional prioritization for historical traces of agency</div> <div class="author"> <em>Michael Lopez-Brau</em>, <a href="https://claracolombatto.com/" rel="external nofollow noopener" target="_blank">Clara Colombatto</a>, <a href="https://psychology.yale.edu/people/julian-jara-ettinger" rel="external nofollow noopener" target="_blank">Julian Jara-Ettinger</a>, and <a href="https://perception.yale.edu/Brian/" rel="external nofollow noopener" target="_blank">Brian Scholl</a> </div> <div class="periodical"> <em>Journal of Vision</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.1167/jov.21.9.2748"></span> <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.1167/jov.21.9.2748" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Among the most important stimuli we can perceive are other agents. Accordingly, a great deal of work has shown how visual attention is prioritized not just for certain lower-level properties (e.g. brightness or motion) but also for *social* stimuli (e.g. our impressive efficiency at detecting the presence of people in natural scenes). In nearly all such work, the relevant agents are explicitly visible — e.g. in the form of bodies, faces, or eyes. But we can also readily perceive the *historical traces* that agents may leave behind. When walking along a hiking trail, for example, a stack of rocks along the side of the path may elicit the immediate strong impression that an agent had been present, since such configurations are exceptionally unlikely to be produced by natural processes. Does visual processing also prioritize such ’traces of agency’ (independent from properties such as order and complexity)? We explored this using visual search, in scenes filled with two kinds of block towers. In Agentic Trace towers, the blocks were slightly misaligned (as would only likely occur if they had been intentionally stacked by an agent), while in Non-Agentic towers they were perfectly stacked (in ways an agent would be unlikely to achieve). Across multiple experiments, observers were both faster and more accurate at detecting Agentic Trace towers (in arrays of Non-Agentic towers), compared to detecting Non-Agentic towers (in arrays of Agentic Trace towers). Critically, this difference was stronger than when the same stimuli were presented in ways that equated order and complexity (e.g. with additional vertical spacing), while eliminating perceived traces of agency. This attentional prioritization for "agency without agents" reveals that social perception is not just a response to the superficial appearances of agents themselves, but also to the deeper and subtler traces that they leave in the world.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lopezbrau2021attentional</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Attentional prioritization for historical traces of agency}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lopez-Brau, Michael and Colombatto, Clara and Jara-Ettinger, Julian and Scholl, Brian}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Vision}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{21}</span><span class="p">,</span>
  <span class="na">issue</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2748--2748}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{The Association for Research in Vision and Ophthalmology}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1167/jov.21.9.2748}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/physical_evidence.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="physical_evidence.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jacobs2021what" class="col-sm-8"> <div class="title">What happened here? Children integrate physical reasoning to infer actions from indirect evidence</div> <div class="author"> Colin Jacobs, <em>Michael Lopez-Brau</em>, and <a href="https://psychology.yale.edu/people/julian-jara-ettinger" rel="external nofollow noopener" target="_blank">Julian Jara-Ettinger</a> </div> <div class="periodical"> <em>Proceedings of the 43rd Annual Meeting of the Cognitive Science Society</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://lopez-brau.github.io/assets/pdf/Jacobs%20et%20al.%20(2021).pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>As we navigate through the world, we often leave traces of our actions: a broken branch, a footprint in the mud, a dirty coffee mug at a desk. As observers, these traces enable us to make surprisingly complex social inferences about the actions that may have caused them: what the other person may have been doing, what their likely goals were, and more. But how might a conspicuous lack of evidence prompt similar reasoning? We hypothesize that children consider the presence and absence of physical evidence to infer possible prior actions and their outcomes. To test this hypothesis, we ask children to infer which of two bowls (each containing different materials) was acted upon without witnessing the action directly. In support of this proposal, we found that children readily reconstruct an agent’s actions after observing indirect evidence. Importantly, they are also able to use the difficulty of concealing such evidence to interpret its absence.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">jacobs2021what</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{What happened here? Children integrate physical reasoning to infer actions from indirect evidence}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jacobs, Colin and Lopez-Brau, Michael and Jara-Ettinger, Julian}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the 43rd Annual Meeting of the Cognitive Science Society}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{43}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/subordinate_class_interpretation.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="subordinate_class_interpretation.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tessler2017warm" class="col-sm-8"> <div class="title">Warm (for winter): Comparison class understanding in vague language</div> <div class="author"> Michael Henry Tessler, <em>Michael Lopez-Brau</em>, and Noah D. Goodman</div> <div class="periodical"> <em>Proceedings of the 39th Annual Meeting of the Cognitive Science Society</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://lopez-brau.github.io/assets/pdf/Tessler%20et%20al.%20(2017).pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/mhtess/comparison-class-cogsci17/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Speakers often refer to context only implicitly when using language. The utterance "it’s warm outside" could signal it’s warm relative to other days of the year or just relative to the current season (e.g., it’s warm for winter). Warm vaguely conveys that the temperature is high relative to some contextual comparison class, but little is known about how a listener decides upon such a standard of comparison. Here, we formalize how world knowledge and listeners’ internal models of speech production can drive the resolution of a comparison class in context. We introduce a Rational Speech Act model and derive two novel predictions from it, which we validate using a paraphrase experiment to measure listeners’ beliefs about the likely comparison class used by a speaker. Our model makes quantitative predictions given prior world knowledge for the domains in question. We triangulate this knowledge with a follow-up language task in the same domains, using Bayesian data analysis to infer priors from both data sets.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tessler2017warm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Warm (for winter): Comparison class understanding in vague language}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tessler, Michael Henry and Lopez-Brau, Michael and Goodman, Noah D.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the 39th Annual Meeting of the Cognitive Science Society}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{39}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>